# Whisper VAD + åˆ†æ®µä¸¦è¡Œå„ªåŒ–æ–¹æ¡ˆ

**ç‰ˆæœ¬**: 1.0
**æ—¥æœŸ**: 2025-01-29
**ç‹€æ…‹**: å¾…å¯¦ä½œ

---

## ğŸ“‹ ç›®æ¨™

å°‡ Whisper éŸ³æª”è½‰éŒ„é€Ÿåº¦å¾ **0.92x**ï¼ˆMedium æ¨¡å‹ï¼‰å„ªåŒ–è‡³ **0.08-0.15x**ï¼Œå¯¦ç¾ **6-12å€æé€Ÿ**ï¼ŒåŒæ™‚ï¼š
- âœ… é›¶é¡å¤–æˆæœ¬
- âœ… æå‡è½‰éŒ„å“è³ªï¼ˆ92-93/100ï¼‰
- âœ… æ”¹å–„èªªè©±è€…è¾¨è­˜æº–ç¢ºåº¦
- âœ… æ”¯æ´é•·éŸ³æª”ï¼ˆ>60åˆ†é˜ï¼‰

---

## ğŸ¯ å„ªåŒ–ç­–ç•¥

### ä¸‰éšæ®µå„ªåŒ–

```
éšæ®µ 1: VAD åŸºç¤å„ªåŒ–        â†’ 1.5-2x æé€Ÿ
éšæ®µ 2: åˆ†æ®µä¸¦è¡Œè™•ç†        â†’ 6-12x æé€Ÿï¼ˆç¸½è¨ˆï¼‰
éšæ®µ 3: GPU å¿«é€Ÿé€šé“ï¼ˆå¯é¸ï¼‰â†’ 30-90x æé€Ÿï¼ˆç¸½è¨ˆï¼‰
```

---

## ğŸ“ ç³»çµ±æ¶æ§‹

### æ•´é«”æµç¨‹

```mermaid
graph TD
    A[éŸ³æª”ä¸Šå‚³] --> B[VAD é è™•ç†]
    B --> C[éœéŸ³æª¢æ¸¬ & ç§»é™¤]
    C --> D[æ™ºèƒ½åˆ†æ®µ]
    D --> E[ä¸¦è¡Œè½‰éŒ„]
    E --> F[çµæœåˆä½µ]
    F --> G[èªªè©±è€…è¾¨è­˜]
    G --> H[æ™‚é–“æˆ³æ ¡æ­£]
    H --> I[è¼¸å‡ºçµæœ]
```

### è©³ç´°æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Audio Upload Service                  â”‚
â”‚                   (Cloud Storage Trigger)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Transcription Orchestrator                  â”‚
â”‚                   (Cloud Run Service)                    â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Step 1: VAD Preprocessing                       â”‚  â”‚
â”‚  â”‚  - Silero VAD èªéŸ³æª¢æ¸¬                            â”‚  â”‚
â”‚  â”‚  - ç§»é™¤éœéŸ³ç‰‡æ®µ                                   â”‚  â”‚
â”‚  â”‚  - ç”ŸæˆèªéŸ³æ™‚é–“æˆ³åˆ—è¡¨                             â”‚  â”‚
â”‚  â”‚  Time: ~1-2ç§’ (60minéŸ³æª”)                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Step 2: Intelligent Chunking                    â”‚  â”‚
â”‚  â”‚  - æŒ‰ VAD çµæœçš„è‡ªç„¶åœé “é»åˆ‡åˆ†                    â”‚  â”‚
â”‚  â”‚  - ç›®æ¨™ç‰‡æ®µé•·åº¦: 10-15åˆ†é˜                        â”‚  â”‚
â”‚  â”‚  - ä¿ç•™ç‰‡æ®µé‡ç–Š: 1-2ç§’ï¼ˆé¿å…åˆ‡æ–·å¥å­ï¼‰            â”‚  â”‚
â”‚  â”‚  - ç”Ÿæˆç‰‡æ®µå…ƒæ•¸æ“šï¼ˆæ™‚é–“ç¯„åœã€é †åºï¼‰               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Step 3: Parallel Transcription                  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚  â”‚ Chunk 1 â”‚ â”‚ Chunk 2 â”‚ â”‚ Chunk N â”‚  (ä¸¦è¡Œ)    â”‚  â”‚
â”‚  â”‚  â”‚ Worker  â”‚ â”‚ Worker  â”‚ â”‚ Worker  â”‚            â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â”‚                                                   â”‚  â”‚
â”‚  â”‚  é…ç½®:                                            â”‚  â”‚
â”‚  â”‚  - Model: faster-whisper medium                  â”‚  â”‚
â”‚  â”‚  - Device: CPU (int8)                            â”‚  â”‚
â”‚  â”‚  - VAD: enabled                                  â”‚  â”‚
â”‚  â”‚  - Batch size: 16                                â”‚  â”‚
â”‚  â”‚  - Max workers: 6                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Step 4: Results Merging                         â”‚  â”‚
â”‚  â”‚  - æŒ‰é †åºåˆä½µæ‰€æœ‰ç‰‡æ®µ                             â”‚  â”‚
â”‚  â”‚  - æ™‚é–“æˆ³èª¿æ•´ï¼ˆè€ƒæ…® VAD ç§»é™¤çš„éœéŸ³ï¼‰              â”‚  â”‚
â”‚  â”‚  - å»é™¤é‡ç–Šéƒ¨åˆ†çš„é‡è¤‡                             â”‚  â”‚
â”‚  â”‚  - ç”Ÿæˆå®Œæ•´é€å­—ç¨¿                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Step 5: Speaker Diarization (Optional)          â”‚  â”‚
â”‚  â”‚  - pyannote.audio 3.1                            â”‚  â”‚
â”‚  â”‚  - ä½¿ç”¨ VAD çµæœè¼”åŠ©                              â”‚  â”‚
â”‚  â”‚  - ç‚ºæ¯å€‹èªéŸ³æ®µæ¨™è¨»èªªè©±è€…                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Analysis Service                        â”‚
â”‚              (Multi-Agent Processing)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ æŠ€è¡“å¯¦ä½œè¦æ ¼

### 1. VAD é è™•ç†æ¨¡çµ„

**ç›®çš„**: æª¢æ¸¬èªéŸ³æ´»å‹•ï¼Œç§»é™¤éœéŸ³ç‰‡æ®µ

**å¯¦ä½œ**:
```python
from faster_whisper import WhisperModel
from silero_vad import load_silero_vad, get_speech_timestamps
import torch

class VADProcessor:
    """VAD èªéŸ³æ´»å‹•æª¢æ¸¬è™•ç†å™¨"""

    def __init__(self):
        self.vad_model = load_silero_vad()

    def detect_speech_segments(
        self,
        audio_path: str,
        threshold: float = 0.5,
        min_speech_duration_ms: int = 250,
        min_silence_duration_ms: int = 500,
        window_size_samples: int = 512,
        speech_pad_ms: int = 400
    ) -> List[Dict]:
        """
        æª¢æ¸¬éŸ³æª”ä¸­çš„èªéŸ³ç‰‡æ®µ

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            threshold: èªéŸ³æª¢æ¸¬é–¾å€¼ (0-1)
            min_speech_duration_ms: æœ€çŸ­èªéŸ³é•·åº¦ï¼ˆæ¯«ç§’ï¼‰
            min_silence_duration_ms: æœ€çŸ­éœéŸ³é•·åº¦ï¼ˆæ¯«ç§’ï¼‰
            window_size_samples: VAD çª—å£å¤§å°
            speech_pad_ms: èªéŸ³å‰å¾Œä¿ç•™æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            List[Dict]: [
                {"start": 0.0, "end": 10.5, "duration": 10.5},
                {"start": 12.0, "end": 25.3, "duration": 13.3},
                ...
            ]
        """
        # è¼‰å…¥éŸ³æª”
        audio_tensor, sample_rate = self._load_audio(audio_path)

        # VAD æª¢æ¸¬
        speech_timestamps = get_speech_timestamps(
            audio_tensor,
            self.vad_model,
            threshold=threshold,
            min_speech_duration_ms=min_speech_duration_ms,
            min_silence_duration_ms=min_silence_duration_ms,
            window_size_samples=window_size_samples,
            speech_pad_ms=speech_pad_ms,
            return_seconds=True
        )

        # æ ¼å¼åŒ–è¼¸å‡º
        segments = [
            {
                "start": ts["start"],
                "end": ts["end"],
                "duration": ts["end"] - ts["start"]
            }
            for ts in speech_timestamps
        ]

        # è¨ˆç®—çµ±è¨ˆ
        total_duration = audio_tensor.shape[0] / sample_rate
        speech_duration = sum(s["duration"] for s in segments)
        silence_duration = total_duration - speech_duration

        print(f"VAD Analysis:")
        print(f"  Total: {total_duration:.1f}s")
        print(f"  Speech: {speech_duration:.1f}s ({speech_duration/total_duration*100:.1f}%)")
        print(f"  Silence: {silence_duration:.1f}s ({silence_duration/total_duration*100:.1f}%)")

        return segments

    def _load_audio(self, audio_path: str):
        """è¼‰å…¥éŸ³æª”ç‚º tensor"""
        import torchaudio
        waveform, sample_rate = torchaudio.load(audio_path)

        # è½‰æ›ç‚ºå–®è²é“
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        # é‡æ¡æ¨£è‡³ 16kHz (VAD éœ€æ±‚)
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
            sample_rate = 16000

        return waveform.squeeze(), sample_rate
```

**æ•ˆèƒ½é ä¼°**:
- è™•ç†é€Ÿåº¦: ~8000x å³æ™‚ï¼ˆ60åˆ†é˜éŸ³æª” <1ç§’ï¼‰
- è¨˜æ†¶é«”: ~100MB
- æº–ç¢ºåº¦: >95%

---

### 2. æ™ºèƒ½åˆ†æ®µæ¨¡çµ„

**ç›®çš„**: å°‡é•·éŸ³æª”æŒ‰ VAD çµæœçš„è‡ªç„¶åœé “é»åˆ‡åˆ†

**å¯¦ä½œ**:
```python
from typing import List, Dict

class AudioChunker:
    """éŸ³æª”æ™ºèƒ½åˆ†æ®µå™¨"""

    def __init__(
        self,
        target_chunk_duration: int = 600,  # 10åˆ†é˜
        max_chunk_duration: int = 900,     # 15åˆ†é˜
        overlap_duration: int = 2          # 2ç§’é‡ç–Š
    ):
        self.target_duration = target_chunk_duration
        self.max_duration = max_chunk_duration
        self.overlap = overlap_duration

    def create_chunks(
        self,
        vad_segments: List[Dict],
        total_duration: float
    ) -> List[Dict]:
        """
        åŸºæ–¼ VAD çµæœå‰µå»ºæ™ºèƒ½åˆ†æ®µ

        Args:
            vad_segments: VAD æª¢æ¸¬çš„èªéŸ³ç‰‡æ®µ
            total_duration: éŸ³æª”ç¸½é•·åº¦ï¼ˆç§’ï¼‰

        Returns:
            List[Dict]: [
                {
                    "chunk_id": 0,
                    "start": 0.0,
                    "end": 612.5,
                    "duration": 612.5,
                    "vad_segments": [...],  # æ­¤ç‰‡æ®µå…§çš„ VAD æ®µ
                    "has_overlap_start": False,
                    "has_overlap_end": True
                },
                ...
            ]
        """
        chunks = []
        current_start = 0.0
        chunk_id = 0

        while current_start < total_duration:
            # è¨ˆç®—ç›®æ¨™çµæŸæ™‚é–“
            target_end = min(
                current_start + self.target_duration,
                total_duration
            )

            # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ¨™çµæŸæ™‚é–“çš„ VAD é–“éš™ï¼ˆéœéŸ³é»ï¼‰
            split_point = self._find_best_split_point(
                vad_segments,
                current_start,
                target_end,
                self.max_duration
            )

            # å‰µå»ºç‰‡æ®µ
            chunk = {
                "chunk_id": chunk_id,
                "start": current_start,
                "end": split_point,
                "duration": split_point - current_start,
                "vad_segments": self._filter_vad_segments(
                    vad_segments,
                    current_start,
                    split_point
                ),
                "has_overlap_start": chunk_id > 0,
                "has_overlap_end": split_point < total_duration
            }

            chunks.append(chunk)

            # ä¸‹ä¸€å€‹ç‰‡æ®µèµ·é»ï¼ˆåŒ…å«é‡ç–Šï¼‰
            current_start = split_point - self.overlap if split_point < total_duration else split_point
            chunk_id += 1

        print(f"Created {len(chunks)} chunks:")
        for c in chunks:
            print(f"  Chunk {c['chunk_id']}: {c['start']:.1f}s - {c['end']:.1f}s ({c['duration']:.1f}s)")

        return chunks

    def _find_best_split_point(
        self,
        vad_segments: List[Dict],
        start: float,
        target_end: float,
        max_end: float
    ) -> float:
        """æ‰¾åˆ°æœ€ä½³åˆ‡åˆ†é»ï¼ˆVAD é–“éš™ï¼‰"""

        # åœ¨ç›®æ¨™çµæŸæ™‚é–“é™„è¿‘ Â±30ç§’ ç¯„åœå…§æ‰¾éœéŸ³é»
        search_window = 30
        search_start = max(start, target_end - search_window)
        search_end = min(max_end, target_end + search_window)

        # æ‰¾åˆ°æ‰€æœ‰ VAD é–“éš™
        gaps = []
        for i in range(len(vad_segments) - 1):
            gap_start = vad_segments[i]["end"]
            gap_end = vad_segments[i + 1]["start"]

            if search_start <= gap_start <= search_end:
                gap_mid = (gap_start + gap_end) / 2
                gap_duration = gap_end - gap_start

                # åå¥½è¼ƒé•·çš„éœéŸ³é–“éš™
                score = gap_duration * 10
                # åå¥½æ¥è¿‘ç›®æ¨™æ™‚é–“çš„é»
                distance_penalty = abs(gap_mid - target_end)
                score -= distance_penalty

                gaps.append({
                    "position": gap_mid,
                    "duration": gap_duration,
                    "score": score
                })

        if gaps:
            # é¸æ“‡å¾—åˆ†æœ€é«˜çš„é–“éš™
            best_gap = max(gaps, key=lambda x: x["score"])
            return best_gap["position"]
        else:
            # æ²’æ‰¾åˆ°åˆé©é–“éš™ï¼Œä½¿ç”¨ç›®æ¨™æ™‚é–“
            return min(target_end, max_end)

    def _filter_vad_segments(
        self,
        vad_segments: List[Dict],
        start: float,
        end: float
    ) -> List[Dict]:
        """éæ¿¾å‡ºæŒ‡å®šæ™‚é–“ç¯„åœå…§çš„ VAD æ®µ"""
        filtered = []
        for seg in vad_segments:
            # æª¢æŸ¥æ˜¯å¦æœ‰é‡ç–Š
            if seg["end"] > start and seg["start"] < end:
                # èª¿æ•´æ™‚é–“ç¯„åœï¼ˆç›¸å°æ–¼ç‰‡æ®µèµ·é»ï¼‰
                adjusted_seg = {
                    "start": max(0, seg["start"] - start),
                    "end": min(end - start, seg["end"] - start),
                    "duration": 0  # ç¨å¾Œè¨ˆç®—
                }
                adjusted_seg["duration"] = adjusted_seg["end"] - adjusted_seg["start"]
                filtered.append(adjusted_seg)

        return filtered
```

**æ•ˆèƒ½**:
- è™•ç†é€Ÿåº¦: <1ç§’
- åˆ†æ®µæ•¸é‡: 60åˆ†é˜éŸ³æª”ç´„ 6-8 æ®µ

---

### 3. ä¸¦è¡Œè½‰éŒ„æ¨¡çµ„

**ç›®çš„**: ä¸¦è¡Œè™•ç†æ‰€æœ‰éŸ³æª”ç‰‡æ®µ

**å¯¦ä½œ**:
```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from faster_whisper import WhisperModel
import time

class ParallelTranscriber:
    """ä¸¦è¡Œè½‰éŒ„è™•ç†å™¨"""

    def __init__(
        self,
        model_size: str = "medium",
        device: str = "cpu",
        compute_type: str = "int8",
        max_workers: int = 6
    ):
        self.model_size = model_size
        self.device = device
        self.compute_type = compute_type
        self.max_workers = max_workers

        # æ¯å€‹ worker æœ‰è‡ªå·±çš„æ¨¡å‹å¯¦ä¾‹
        self.model_pool = []

    def transcribe_chunks(
        self,
        audio_path: str,
        chunks: List[Dict]
    ) -> List[Dict]:
        """
        ä¸¦è¡Œè½‰éŒ„æ‰€æœ‰ç‰‡æ®µ

        Args:
            audio_path: åŸå§‹éŸ³æª”è·¯å¾‘
            chunks: åˆ†æ®µè³‡è¨Šåˆ—è¡¨

        Returns:
            List[Dict]: æ¯å€‹ç‰‡æ®µçš„è½‰éŒ„çµæœ
        """
        print(f"Starting parallel transcription with {self.max_workers} workers...")
        start_time = time.time()

        # ä½¿ç”¨ ThreadPoolExecutor ä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_chunk = {
                executor.submit(
                    self._transcribe_single_chunk,
                    audio_path,
                    chunk
                ): chunk
                for chunk in chunks
            }

            # æ”¶é›†çµæœ
            results = []
            for future in as_completed(future_to_chunk):
                chunk = future_to_chunk[future]
                try:
                    result = future.result()
                    results.append(result)
                    print(f"  âœ… Chunk {chunk['chunk_id']} completed in {result['processing_time']:.1f}s")
                except Exception as e:
                    print(f"  âŒ Chunk {chunk['chunk_id']} failed: {e}")
                    results.append({
                        "chunk_id": chunk["chunk_id"],
                        "error": str(e),
                        "success": False
                    })

        # æŒ‰ chunk_id æ’åº
        results.sort(key=lambda x: x["chunk_id"])

        total_time = time.time() - start_time
        successful = sum(1 for r in results if r.get("success", False))

        print(f"\nParallel transcription completed:")
        print(f"  Total time: {total_time:.1f}s")
        print(f"  Successful: {successful}/{len(chunks)}")
        print(f"  Average time per chunk: {total_time/len(chunks):.1f}s")

        return results

    def _transcribe_single_chunk(
        self,
        audio_path: str,
        chunk: Dict
    ) -> Dict:
        """è½‰éŒ„å–®ä¸€ç‰‡æ®µ"""

        # è¼‰å…¥æ¨¡å‹ï¼ˆæ¯å€‹ worker ä¸€å€‹å¯¦ä¾‹ï¼‰
        model = WhisperModel(
            self.model_size,
            device=self.device,
            compute_type=self.compute_type
        )

        # æå–éŸ³æª”ç‰‡æ®µ
        chunk_audio = self._extract_audio_segment(
            audio_path,
            chunk["start"],
            chunk["end"]
        )

        # è½‰éŒ„
        start_time = time.time()

        segments, info = model.transcribe(
            chunk_audio,
            language="zh",

            # VAD åƒæ•¸
            vad_filter=True,
            vad_parameters={
                "threshold": 0.5,
                "min_speech_duration_ms": 250,
                "min_silence_duration_ms": 500,
                "window_size_samples": 512,
                "speech_pad_ms": 400
            },

            # å„ªåŒ–åƒæ•¸
            batch_size=16,
            beam_size=5,
            best_of=5,
            temperature=0.0
        )

        # æ”¶é›†çµæœ
        transcript_segments = []
        for segment in segments:
            transcript_segments.append({
                "start": segment.start + chunk["start"],  # èª¿æ•´ç‚ºå…¨å±€æ™‚é–“
                "end": segment.end + chunk["start"],
                "text": segment.text,
                "confidence": segment.avg_logprob
            })

        processing_time = time.time() - start_time

        return {
            "chunk_id": chunk["chunk_id"],
            "success": True,
            "chunk_start": chunk["start"],
            "chunk_end": chunk["end"],
            "segments": transcript_segments,
            "processing_time": processing_time,
            "language": info.language,
            "language_probability": info.language_probability
        }

    def _extract_audio_segment(
        self,
        audio_path: str,
        start: float,
        end: float
    ) -> str:
        """æå–éŸ³æª”ç‰‡æ®µä¸¦å„²å­˜ç‚ºè‡¨æ™‚æª”æ¡ˆ"""
        import tempfile
        from pydub import AudioSegment

        # è¼‰å…¥éŸ³æª”
        audio = AudioSegment.from_file(audio_path)

        # æå–ç‰‡æ®µï¼ˆpydub ä½¿ç”¨æ¯«ç§’ï¼‰
        segment = audio[int(start * 1000):int(end * 1000)]

        # å„²å­˜ç‚ºè‡¨æ™‚æª”æ¡ˆ
        temp_file = tempfile.NamedTemporaryFile(
            suffix=".wav",
            delete=False
        )
        segment.export(temp_file.name, format="wav")

        return temp_file.name
```

**æ•ˆèƒ½**:
- 6 å€‹ä¸¦è¡Œ workers
- 60åˆ†é˜éŸ³æª”ï¼ˆ6æ®µï¼‰: ç¸½è™•ç†æ™‚é–“ ~4-6åˆ†é˜
- è¨˜æ†¶é«”: ~2GB (6 workers Ã— ~300MB)

---

### 4. çµæœåˆä½µæ¨¡çµ„

**ç›®çš„**: åˆä½µæ‰€æœ‰ç‰‡æ®µçš„è½‰éŒ„çµæœ

**å¯¦ä½œ**:
```python
class TranscriptMerger:
    """è½‰éŒ„çµæœåˆä½µå™¨"""

    def __init__(self, overlap_duration: float = 2.0):
        self.overlap = overlap_duration

    def merge_chunks(
        self,
        chunk_results: List[Dict]
    ) -> Dict:
        """
        åˆä½µæ‰€æœ‰ç‰‡æ®µçš„è½‰éŒ„çµæœ

        Args:
            chunk_results: å„ç‰‡æ®µçš„è½‰éŒ„çµæœ

        Returns:
            Dict: å®Œæ•´çš„è½‰éŒ„çµæœ
        """
        all_segments = []

        for i, chunk in enumerate(chunk_results):
            if not chunk.get("success", False):
                print(f"âš ï¸  Skipping failed chunk {chunk['chunk_id']}")
                continue

            segments = chunk["segments"]

            # è™•ç†é‡ç–Šéƒ¨åˆ†
            if i > 0 and self.overlap > 0:
                # ç§»é™¤å‰ä¸€å€‹ç‰‡æ®µé‡ç–Šéƒ¨åˆ†çš„å…§å®¹
                overlap_start = chunk["chunk_start"]
                overlap_end = overlap_start + self.overlap

                # ç§»é™¤ç•¶å‰ç‰‡æ®µé‡ç–Šå€åŸŸçš„ segments
                segments = [
                    s for s in segments
                    if s["start"] >= overlap_end
                ]

            all_segments.extend(segments)

        # æŒ‰æ™‚é–“æ’åº
        all_segments.sort(key=lambda x: x["start"])

        # ç”Ÿæˆå®Œæ•´æ–‡å­—
        full_text = " ".join(s["text"] for s in all_segments)

        # è¨ˆç®—çµ±è¨ˆ
        total_duration = all_segments[-1]["end"] if all_segments else 0
        avg_confidence = sum(s["confidence"] for s in all_segments) / len(all_segments) if all_segments else 0

        return {
            "segments": all_segments,
            "full_text": full_text,
            "total_segments": len(all_segments),
            "total_duration": total_duration,
            "average_confidence": avg_confidence,
            "chunks_processed": len([c for c in chunk_results if c.get("success", False)]),
            "chunks_failed": len([c for c in chunk_results if not c.get("success", False)])
        }
```

---

### 5. å®Œæ•´æµç¨‹æ•´åˆ

**ä¸»è¦è™•ç†å‡½æ•¸**:
```python
class OptimizedTranscriptionPipeline:
    """å„ªåŒ–çš„è½‰éŒ„æµç¨‹"""

    def __init__(self):
        self.vad_processor = VADProcessor()
        self.chunker = AudioChunker(
            target_chunk_duration=600,  # 10åˆ†é˜
            overlap_duration=2
        )
        self.transcriber = ParallelTranscriber(
            model_size="medium",
            max_workers=6
        )
        self.merger = TranscriptMerger()

    def process_audio(self, audio_path: str) -> Dict:
        """
        å®Œæ•´çš„éŸ³æª”è™•ç†æµç¨‹

        Args:
            audio_path: éŸ³æª”è·¯å¾‘

        Returns:
            Dict: å®Œæ•´çš„è½‰éŒ„çµæœ
        """
        print(f"Processing audio: {audio_path}")
        pipeline_start = time.time()

        # Step 1: VAD é è™•ç†
        print("\n[1/4] VAD preprocessing...")
        vad_start = time.time()
        vad_segments = self.vad_processor.detect_speech_segments(audio_path)
        vad_time = time.time() - vad_start
        print(f"  Completed in {vad_time:.1f}s")

        # Step 2: æ™ºèƒ½åˆ†æ®µ
        print("\n[2/4] Creating intelligent chunks...")
        chunk_start = time.time()

        # ç²å–éŸ³æª”ç¸½é•·åº¦
        from pydub import AudioSegment
        audio = AudioSegment.from_file(audio_path)
        total_duration = len(audio) / 1000.0

        chunks = self.chunker.create_chunks(vad_segments, total_duration)
        chunk_time = time.time() - chunk_start
        print(f"  Completed in {chunk_time:.1f}s")

        # Step 3: ä¸¦è¡Œè½‰éŒ„
        print("\n[3/4] Parallel transcription...")
        transcribe_start = time.time()
        chunk_results = self.transcriber.transcribe_chunks(audio_path, chunks)
        transcribe_time = time.time() - transcribe_start

        # Step 4: åˆä½µçµæœ
        print("\n[4/4] Merging results...")
        merge_start = time.time()
        final_result = self.merger.merge_chunks(chunk_results)
        merge_time = time.time() - merge_start
        print(f"  Completed in {merge_time:.1f}s")

        # ç¸½çµ
        total_time = time.time() - pipeline_start
        speed_ratio = total_time / total_duration

        print(f"\n{'='*60}")
        print(f"Pipeline Summary:")
        print(f"  Audio duration: {total_duration:.1f}s ({total_duration/60:.1f} min)")
        print(f"  Processing time: {total_time:.1f}s ({total_time/60:.1f} min)")
        print(f"  Speed ratio: {speed_ratio:.3f}x")
        print(f"  Speedup: {1/speed_ratio:.1f}x faster than real-time")
        print(f"\n  Time breakdown:")
        print(f"    VAD: {vad_time:.1f}s ({vad_time/total_time*100:.1f}%)")
        print(f"    Chunking: {chunk_time:.1f}s ({chunk_time/total_time*100:.1f}%)")
        print(f"    Transcription: {transcribe_time:.1f}s ({transcribe_time/total_time*100:.1f}%)")
        print(f"    Merging: {merge_time:.1f}s ({merge_time/total_time*100:.1f}%)")
        print(f"{'='*60}")

        # æ·»åŠ å…ƒæ•¸æ“š
        final_result["processing_metadata"] = {
            "audio_duration": total_duration,
            "total_processing_time": total_time,
            "speed_ratio": speed_ratio,
            "vad_time": vad_time,
            "chunking_time": chunk_time,
            "transcription_time": transcribe_time,
            "merging_time": merge_time,
            "num_chunks": len(chunks),
            "num_vad_segments": len(vad_segments)
        }

        return final_result


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    pipeline = OptimizedTranscriptionPipeline()
    result = pipeline.process_audio("sales_call_60min.m4a")

    # å„²å­˜çµæœ
    import json
    with open("transcript_output.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
```

---

## ğŸ“Š æ•ˆèƒ½é ä¼°

### è™•ç†æ™‚é–“å°æ¯”

| éŸ³æª”é•·åº¦ | æœªå„ªåŒ– | VAD only | VAD + åˆ†æ®µ | æé€Ÿæ¯” |
|---------|--------|----------|-----------|--------|
| 30åˆ†é˜ | 28åˆ†é˜ | 18åˆ†é˜ | **2-3åˆ†é˜** | 9-14x |
| 60åˆ†é˜ | 55åˆ†é˜ | 36åˆ†é˜ | **4-6åˆ†é˜** | 9-14x |
| 90åˆ†é˜ | 83åˆ†é˜ | 54åˆ†é˜ | **6-9åˆ†é˜** | 9-14x |
| 120åˆ†é˜ | 110åˆ†é˜ | 72åˆ†é˜ | **8-12åˆ†é˜** | 9-14x |

### å“è³ªæŒ‡æ¨™

| æŒ‡æ¨™ | æœªå„ªåŒ– | å„ªåŒ–å¾Œ | è®ŠåŒ– |
|------|--------|--------|------|
| WER (éŒ¯èª¤ç‡) | 8-10% | **6-8%** | â†“ 20-25% |
| å“è³ªåˆ†æ•¸ | 91.6 | **92-93** | â†‘ 0.4-1.4 |
| DER (èªªè©±è€…éŒ¯èª¤) | 35-45% | **17-25%** | â†“ 40-60% |

### è³‡æºä½¿ç”¨

| è³‡æº | å–®æ®µè™•ç† | ä¸¦è¡Œè™•ç† (6 workers) |
|------|---------|---------------------|
| CPU | 1 æ ¸ 100% | 6 æ ¸ 80-90% |
| è¨˜æ†¶é«” | ~400MB | ~2GB |
| ç£ç¢Ÿ I/O | ä½ | ä¸­ç­‰ |

---

## ğŸš€ éƒ¨ç½²é…ç½®

### Cloud Run é…ç½®

```yaml
# cloud-run-transcription-service.yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: transcription-service
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/minScale: "0"
        autoscaling.knative.dev/maxScale: "10"
    spec:
      containerConcurrency: 1  # ä¸€æ¬¡è™•ç†ä¸€å€‹è«‹æ±‚
      timeoutSeconds: 3600     # 1å°æ™‚è¶…æ™‚ï¼ˆè™•ç†é•·éŸ³æª”ï¼‰
      containers:
      - image: gcr.io/PROJECT_ID/transcription-service:latest
        resources:
          limits:
            cpu: "8"      # 8 vCPU (æ”¯æ´ 6 workers + overhead)
            memory: "16Gi" # 16GB RAM
        env:
        - name: WHISPER_MODEL
          value: "medium"
        - name: MAX_WORKERS
          value: "6"
        - name: CHUNK_DURATION
          value: "600"
```

### Dockerfile

```dockerfile
FROM python:3.11-slim

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£ Python å¥—ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ä¸‹è¼‰æ¨¡å‹ï¼ˆå»ºç«‹æ™‚é è¼‰ï¼Œé¿å…é¦–æ¬¡å•Ÿå‹•ä¸‹è¼‰ï¼‰
RUN python -c "from faster_whisper import WhisperModel; WhisperModel('medium', device='cpu', compute_type='int8')"
RUN python -c "from silero_vad import load_silero_vad; load_silero_vad()"

# è¤‡è£½ç¨‹å¼ç¢¼
COPY src/ /app/src/
WORKDIR /app

CMD ["python", "-m", "src.main"]
```

### requirements.txt

```
faster-whisper==1.2.0
silero-vad==4.0.0
torch==2.1.0
torchaudio==2.1.0
pydub==0.25.1
google-cloud-storage==2.10.0
google-cloud-firestore==2.14.0
```

---

## ğŸ“… å¯¦ä½œæ™‚ç¨‹

### Phase 1: æ ¸å¿ƒåŠŸèƒ½ï¼ˆ1-2é€±ï¼‰

**Week 1**:
- [ ] VAD é è™•ç†æ¨¡çµ„
- [ ] æ™ºèƒ½åˆ†æ®µæ¨¡çµ„
- [ ] å–®å…ƒæ¸¬è©¦

**Week 2**:
- [ ] ä¸¦è¡Œè½‰éŒ„æ¨¡çµ„
- [ ] çµæœåˆä½µæ¨¡çµ„
- [ ] æ•´åˆæ¸¬è©¦

### Phase 2: å„ªåŒ–èˆ‡æ¸¬è©¦ï¼ˆ1é€±ï¼‰

- [ ] æ•ˆèƒ½æ¸¬è©¦ï¼ˆä¸åŒéŸ³æª”é•·åº¦ï¼‰
- [ ] å“è³ªé©—è­‰ï¼ˆå°æ¯”åŸå§‹ Whisperï¼‰
- [ ] éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶
- [ ] ç›£æ§èˆ‡æ—¥èªŒ

### Phase 3: éƒ¨ç½²ï¼ˆ3-5å¤©ï¼‰

- [ ] Dockerfile å»ºç«‹
- [ ] Cloud Run éƒ¨ç½²
- [ ] CI/CD è¨­å®š
- [ ] ç”Ÿç”¢ç’°å¢ƒæ¸¬è©¦

---

## ğŸ§ª æ¸¬è©¦è¨ˆç•«

### å–®å…ƒæ¸¬è©¦

```python
def test_vad_processor():
    """æ¸¬è©¦ VAD è™•ç†å™¨"""
    processor = VADProcessor()
    segments = processor.detect_speech_segments("test_audio.wav")

    assert len(segments) > 0
    assert all(s["end"] > s["start"] for s in segments)
    assert all(s["duration"] > 0 for s in segments)

def test_audio_chunker():
    """æ¸¬è©¦éŸ³æª”åˆ†æ®µå™¨"""
    chunker = AudioChunker(target_chunk_duration=300)

    vad_segments = [
        {"start": 0, "end": 100, "duration": 100},
        {"start": 120, "end": 500, "duration": 380},
        {"start": 510, "end": 900, "duration": 390}
    ]

    chunks = chunker.create_chunks(vad_segments, total_duration=900)

    assert len(chunks) >= 2
    assert chunks[0]["start"] == 0
    assert chunks[-1]["end"] <= 900
```

### æ•´åˆæ¸¬è©¦

```python
def test_full_pipeline():
    """æ¸¬è©¦å®Œæ•´æµç¨‹"""
    pipeline = OptimizedTranscriptionPipeline()

    # ä½¿ç”¨æ¸¬è©¦éŸ³æª”
    result = pipeline.process_audio("test_sales_call_30min.m4a")

    # é©—è­‰çµæœ
    assert result["total_segments"] > 0
    assert len(result["full_text"]) > 0
    assert result["chunks_failed"] == 0

    # é©—è­‰æ•ˆèƒ½
    metadata = result["processing_metadata"]
    speed_ratio = metadata["speed_ratio"]

    # æ‡‰è©²æ¯”å³æ™‚å¿«ï¼ˆspeed_ratio < 0.2 è¡¨ç¤º 5x å¿«ï¼‰
    assert speed_ratio < 0.2

    print(f"âœ… Processing took {metadata['total_processing_time']:.1f}s")
    print(f"âœ… Speed ratio: {speed_ratio:.3f}x ({1/speed_ratio:.1f}x faster)")
```

### æ•ˆèƒ½åŸºæº–æ¸¬è©¦

```python
def benchmark_different_lengths():
    """æ¸¬è©¦ä¸åŒé•·åº¦éŸ³æª”çš„æ•ˆèƒ½"""
    pipeline = OptimizedTranscriptionPipeline()

    test_files = [
        ("15min_audio.m4a", 900),
        ("30min_audio.m4a", 1800),
        ("60min_audio.m4a", 3600),
        ("90min_audio.m4a", 5400)
    ]

    results = []

    for file_path, expected_duration in test_files:
        result = pipeline.process_audio(file_path)
        metadata = result["processing_metadata"]

        results.append({
            "duration_min": expected_duration / 60,
            "processing_time_min": metadata["total_processing_time"] / 60,
            "speed_ratio": metadata["speed_ratio"],
            "speedup": 1 / metadata["speed_ratio"]
        })

    # è¼¸å‡ºå ±å‘Š
    print("\n" + "="*60)
    print("Performance Benchmark Results")
    print("="*60)
    print(f"{'Duration':<12} {'Process Time':<15} {'Speed Ratio':<12} {'Speedup':<10}")
    print("-"*60)

    for r in results:
        print(f"{r['duration_min']:>8.0f} min  "
              f"{r['processing_time_min']:>12.1f} min  "
              f"{r['speed_ratio']:>10.3f}x  "
              f"{r['speedup']:>8.1f}x")

    print("="*60)
```

---

## ğŸ” ç›£æ§æŒ‡æ¨™

### é—œéµæŒ‡æ¨™

1. **æ•ˆèƒ½æŒ‡æ¨™**:
   - è™•ç†æ™‚é–“ï¼ˆç¸½è¨ˆï¼‰
   - é€Ÿåº¦æ¯” (processing_time / audio_duration)
   - å„éšæ®µæ™‚é–“åˆ†å¸ƒ

2. **å“è³ªæŒ‡æ¨™**:
   - è½‰éŒ„æº–ç¢ºåº¦ï¼ˆæŠ½æ¨£é©—è­‰ï¼‰
   - VAD æª¢æ¸¬æº–ç¢ºåº¦
   - ç‰‡æ®µåˆä½µæ­£ç¢ºæ€§

3. **è³‡æºæŒ‡æ¨™**:
   - CPU ä½¿ç”¨ç‡
   - è¨˜æ†¶é«”ä½¿ç”¨é‡
   - ä¸¦è¡Œ worker æ•¸é‡

4. **æ¥­å‹™æŒ‡æ¨™**:
   - æ¯æ—¥è™•ç†éŸ³æª”æ•¸
   - å¹³å‡éŸ³æª”é•·åº¦
   - å¤±æ•—ç‡

---

## ğŸ“ æœ€ä½³å¯¦è¸

### VAD åƒæ•¸èª¿æ•´å»ºè­°

```python
# æœƒè­°éŒ„éŸ³ï¼ˆå¤šäººå°è©±ï¼‰
vad_params_meeting = {
    "threshold": 0.5,
    "min_speech_duration_ms": 250,  # çŸ­å¥ä¹Ÿè¦ä¿ç•™
    "min_silence_duration_ms": 500, # è¼ƒçŸ­çš„éœéŸ³ä¹Ÿåˆ‡åˆ†
    "speech_pad_ms": 400
}

# æ¼”è¬›/ç°¡å ±ï¼ˆå–®äººé•·æ™‚é–“ï¼‰
vad_params_presentation = {
    "threshold": 0.6,               # æé«˜é–¾å€¼é¿å…èª¤åˆ¤
    "min_speech_duration_ms": 500,  # éæ¿¾æ‰æ¥µçŸ­çš„é›œéŸ³
    "min_silence_duration_ms": 1000,# è¼ƒé•·çš„åœé “æ‰åˆ‡åˆ†
    "speech_pad_ms": 300
}

# å˜ˆé›œç’°å¢ƒï¼ˆèƒŒæ™¯å™ªéŸ³å¤§ï¼‰
vad_params_noisy = {
    "threshold": 0.7,               # æ›´é«˜é–¾å€¼
    "min_speech_duration_ms": 500,
    "min_silence_duration_ms": 800,
    "speech_pad_ms": 500            # æ›´å¤š padding
}
```

### éŒ¯èª¤è™•ç†

```python
# ç‰‡æ®µè™•ç†å¤±æ•—é‡è©¦æ©Ÿåˆ¶
def transcribe_with_retry(chunk, max_retries=3):
    for attempt in range(max_retries):
        try:
            return transcribe_chunk(chunk)
        except Exception as e:
            if attempt == max_retries - 1:
                # æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œè¨˜éŒ„éŒ¯èª¤
                log_error(f"Chunk {chunk['id']} failed after {max_retries} attempts: {e}")
                return None
            else:
                # ç­‰å¾…å¾Œé‡è©¦
                time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
```

---

## ğŸ“š åƒè€ƒè³‡æ–™

- [Faster-Whisper GitHub](https://github.com/SYSTRAN/faster-whisper)
- [Silero VAD Documentation](https://github.com/snakers4/silero-vad)
- [WhisperX Paper](https://arxiv.org/abs/2303.00747)
- [Cloud Run GPU Documentation](https://cloud.google.com/run/docs/configuring/services/gpu)

---

**ç‰ˆæœ¬æ­·å²**:
- v1.0 (2025-01-29): åˆå§‹ç‰ˆæœ¬
